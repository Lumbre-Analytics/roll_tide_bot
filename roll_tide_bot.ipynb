{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll Tide Chatbot\n",
    "This (just for fun) notebook uses OpenAI and langchain to create a simple chat bot which:\n",
    "- Is an unreasonable Alabama fan\n",
    "- Has access to a RAG of Wikipedia pages related to Alabama football."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain, LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting for multi-line string literal output\n",
    "from IPython.core.formatters import BaseFormatter\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "class MultilineStringFormatter(BaseFormatter):\n",
    "    def __call__(self, obj):\n",
    "        if isinstance(obj, str) and '\\n' in obj:\n",
    "            return f'<pre>{obj}</pre>'\n",
    "        return None\n",
    "\n",
    "# Register the custom formatter\n",
    "ip = get_ipython()\n",
    "ip.display_formatter.formatters['text/html'].for_type(str, MultilineStringFormatter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open API key\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our LLM to use, in this case Chat GPT 4\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by passing a simple prompt to Chat GPT, instructing it how to respond to prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_template = \"\"\"\n",
    "    Answer the prompt as a very enthusiastic fan of the Alabama Crimson Tide football team. \n",
    "    If the prompt mentions the Auburn Tigers or Tennessee Volunteers, be sure to say nothing good about those teams.\n",
    "    If the prompt does not mention one of these two teams, you don't have to bring them up.\n",
    "    Prompt: {question}\"\"\"\n",
    "general_prompt = PromptTemplate(template=general_template, input_variables=[\"question\"])\n",
    "general_chain = general_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who do you think will win the college football nation championship in 2024?',\n",
       " 'text': \"Roll Tide, baby! Without a shadow of a doubt, the Alabama Crimson Tide will be the ones hoisting that championship trophy in 2024! Coach Saban's got this team on a roll and there's no stopping us. We've got the best recruits, the best coaching staff, and the most dedicated fan base in the nation. We're going to steamroll right over any team that stands in our way. We're the Crimson Tide, and we're unstoppable!\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_chain.invoke(\"Who do you think will win the college football nation championship in 2024?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Chat GPT understood the task and answered with the right kind of enthusiasm. \n",
    "Unfortunately, like many Alabama fans, it hasn't come to grips with the fact that Coach Saban has retired.\n",
    "\n",
    "Next we can see if we can make it a bit more more knowledgeable by incorporating a RAG chain to access a knowledge base of Wikipedia articles related to Alabama football."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RAG chain\n",
    "\n",
    "First we can creat our knowledge base by downloading some selected articles from Wikipedia, and using OpenAI to embed them, and ChromaDB to store them as vectorstore: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions written separately in utils.py to download and processes the knowledge base\n",
    "from utils import download_wikipedia_pages_by_category, create_vector_db\n",
    "\n",
    "download_wikipedia_pages_by_category(\n",
    "        categories= [\n",
    "            \"Category:Alabama_Crimson_Tide_football_seasons\",\n",
    "            \"Category:Alabama_Crimson_Tide_football\",\n",
    "            \"Category:Alabama_Crimson_Tide_football_games\",\n",
    "            \"Category:Alabama_Crimson_Tide_football_bowl_games\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "create_vector_db(docs_dir=\"docs\", db_dir=\"docs-db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "docs_vectorstore = Chroma(collection_name='docs_store', persist_directory=\"docs-db\", embedding_function=embeddings_model)\n",
    "# Initialize retriever to fetch information from knowledgebase\n",
    "retriever = docs_vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the RAG chain, we'll use a new prompt instructing the LLM to retrieve data from the knowledgebase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"\"\"Use the given context to answer the question.\n",
    "    Your response should be a consice summary of the context.\n",
    "    If you reference a specific game, you need to give the year and opponent of the game in your response.\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we don't want to call the RAG chain in response to every type of prompt. \n",
    "\n",
    "To avoid this, we can add another chain where we instruct the LLM to evaluate whether or not the prompt is relevant to our RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_template = \"\"\"Decide if the following prompt is relevant to a specific data retrieval process:\n",
    "Prompt: {question}\n",
    "Answer with 'yes' if this prompt is a question which requires retrieving specific data from a knowledge base about Alabama football, \n",
    "and 'no' if the question relates to any other topic, including other football teams, other questions about the university of alabama.\n",
    "Also answer 'no' if the question asks you to speculate, or give an opinion\"\"\"\n",
    "\n",
    "relevance_prompt = PromptTemplate(template=relevance_template, input_variables=[\"question\"])\n",
    "relevance_chain = LLMChain(llm=llm, prompt=relevance_prompt)\n",
    "\n",
    "\n",
    "def is_rag_relevant(question):\n",
    "    relevance_response = relevance_chain.invoke({\"question\": question})['text'].strip().lower()\n",
    "    return relevance_response == 'yes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_rag_relevant(\"List the players on Alabama's 2015 team\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_rag_relevant(\"List the players on Auburn's 2015 team\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_rag_relevant(\"Who do you think the best college football team is?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can stitch these pieces together into a function that uses the RAG if a prompt is relevant, and defaults to our initial prompt if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_with_rag(question):\n",
    "    if is_rag_relevant(question):\n",
    "        print(\">>> RAG chain called.\")\n",
    "        response = rag_chain.invoke({\"input\":question})['answer']\n",
    "\n",
    "        # If we fail to find the answer in  the knowledge base, default back to the general prompt.\n",
    "        if \"the context does not provide information\" in response.lower().strip():\n",
    "            response = general_chain.run(question)\n",
    "    else:\n",
    "        response = general_chain.run(question)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> RAG chain called.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The 2011 Alabama Crimson Tide football team, led by head coach Nick Saban, represented the University of Alabama in the 2011 NCAA Division I FBS football season. They were part of the Southeastern Conference and played their home games at Bryant–Denny Stadium in Tuscaloosa, Alabama. The team finished the season with a record of twelve wins and one loss, and were named consensus national champions. Despite losing to the LSU Tigers in their regular season, they were considered a favorite to win the Western Division and compete for the SEC championship.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response_with_rag(\"Summarize Alabama's 2011 season\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> RAG chain called.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Alabama football had its worst season in 1955 with a record of 0–10.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response_with_rag(\"In what season did Alabama have the worst record?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Agent\n",
    "The LLM is now using the knowledge base to retrieve specifc information related to our prompts. In responses where the RAG chain is called, we've lost a lot of it's personality. We can add a reflection chain to evaluate and rewrite responses from the RAG chain to be more in line with the type of homerism we're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_template = \"\"\"\n",
    "Evaluate the following response based on the following criteria:\n",
    "1. Is the response accurate and complete?\n",
    "2. Does the tone of the response match that of an enthusiastic Alabama fan who doesn't like to talk about Alabama losing?\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "If the response does not satisfy the criteria listed above, modify it slightly so that it does match the criteria.\n",
    "Don't add any commentary about the response and whether or not it satisfies the criteria, just return your modified response.\n",
    "\"\"\"\n",
    "reflection_prompt = PromptTemplate(template=reflection_template, input_variables=[\"response\"])\n",
    "reflection_chain = LLMChain(llm=llm, prompt=reflection_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_with_reflection(question):\n",
    "    if is_rag_relevant(question):\n",
    "        response = rag_chain.invoke({\"input\":question})['answer']\n",
    "        \n",
    "         # If we fail to find the answer in  the knowledge base, default back to the general prompt.\n",
    "        if \"the context does not provide information\" in response.lower().strip():\n",
    "            response = general_chain.run(question)\n",
    "        \n",
    "        # Call the reflection agent to modify our response if needed.\n",
    "        final_reponse = reflection_chain.invoke({\"response\": response})['text']\n",
    "        \n",
    "    else:\n",
    "        response = None\n",
    "        final_response = general_chain.run(question)\n",
    "    \n",
    "    return response, final_reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, final_response, = get_response_with_reflection(\"In what season did Alabama have the worst record?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alabama football had its worst season in 1955, going 0–10.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unmodified response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, there was a time when Alabama football had a challenging season back in 1955, but let's focus on all the amazing wins we've had since then! Roll Tide!\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response after passing through the reflection chain:\n",
    "final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our chatbot is accessing the knowledge base to retrieve relevant info, but reflecting on its answers and choosing to gloss over certain facts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
